%!TEX root = ./main.tex

\section{Differential Calculus in $\R^n$}

\subsection{Basics}
\subsubsection{Norm}
For $x \in \R^n$ we use the norm $\left| x \right | = \sqrt{x_1^2 + \dots + x_n^2}$, which satisfies

\begin{inparaitem}
    \item $| x | > 0 \quad \forall x \neq 0$
    \item $| tx | = | t | | x |$
    \item $| x + y | \le | x | + | y |$
\end{inparaitem}

\subsubsection{Set Properties}
Subset $X \subset \R^n$ is:
\begin{compactdesc}
    \item[Bounded:] if $\{|x| | x \in X\}$ is bounded.
    \item[Closed:] if every sequence $(x_k)$ in $X$ for which $\lim_{k \to a} (x_k) = y, y \in \R^n$ we have $x \in X$.
    \item[Compact:] if closed and bounded.
    \item[Open:] if $\forall x \in X, \exists r > 0$ s.t. $\{y \in \R^n | |y - x| < r\} \subset X$.
        \begin{compactitem}
            \item Equivalently, if its complements $\R^n \setminus X$ is closed.
        \end{compactitem}
\end{compactdesc}

\begin{compactitem}
    \item If $X \subset \R^n, Y \subset \R^m$ are bounded (or closed or compact or open), then $X \times Y = \{(x, y) \in \R^{n+m} | x \in X, y \in Y\}$ is bounded (or closed or compact or open).
    \item For $f: \R^n \to \R^m$ continuous, $\forall Y \subset \R^m$ closed, the set $f^{-1}(y) = \{x \in \R^n | f(x) \in Y\} \subset \R^n$ is closed. I.e. the inverse image of a closed set under a continuous map is closed.
\end{compactitem}

\subsubsection{Sequences}
For sequence $(x_k)_k, x_k \in \R^n = (x_{k,1}, \dots x_{k,n})$ and $y \in \R^n = (y_1, \dots y_n)$, $x_k$ converges to $y$ as $k \to \infty$ if $\forall \epsilon > 0, \exists N \ge 1$, s.t. $\forall n \ge N, |x_k - y| < \epsilon$.

We write $x_k \to y$ or $\lim_{k \to \infty} x_k = y$.

I.e. $\lim_{k \to \infty} x_k = y$, if one of the following holds:
\begin{compactitem}
    \item $\forall 1 \le i \le n$, the sequence $(a_{k,i})$ converges to $y_i$.
    \item $\lim_{k \to \infty} |x_k - y| = 0$.
\end{compactitem}

\subsubsection{Limit of $\mathbf{f}$}
Function $f: X \subset \R^n \to \R^m$ has limit $y \in \R^m$ if for $x \to x_0, x_0 \in X, \quad \forall \epsilon > 0, \exists \delta > 0$ s.t. $\forall x \in X, x \neq x_0$, s.t. $|x - x_0| < \epsilon \implies |f(x) - y| < \delta$. In that case $\lim_{x \to x_0} f(x) = y$.

I.e. $\lim_{x\to x_0} f(x) = y \iff \forall$ sequences $(x_k)$ in $X$, where $\lim_{k \to \infty} x_k = x_0$, the sequence $\lim_{k \to \infty} f(x_k) = y$.

\subsubsection{Min-Max-Theorem}
For $X \subset \R^n$ compact and non-empty and for $f: X \to \R$ continuous. Then $f$ is bounded and achieves a maximum $x^+, f(x^+) = \sup_{x \in X} f(x)$ and minimum $x^-, f(x^-) = \inf_{x \in X} f(x)$.

\subsection{Continuity}
\begin{compactdesc}
    \item[at $\mathbf{x_0}$:] for $f: X \subset \R^n \to \R^m, x_0 \in X$ if $\forall \epsilon > 0, \exists \delta > 0$ s.t. for $x \in X$ satisfies $| x - x_0 | < \delta \implies | f(x) - f_0(x) | < \epsilon$.
    \item[on $\textbf{X}$:] for $f: X \subset \R^n \to \R^m$ if $x_0$ continuous $\forall x_0 \in X$.
\end{compactdesc}

Function $f: X \subset \R^n \to \R^m$ is continuous at $x_0 \in X \iff \forall$ sequences $(x_k)$ in $X$, where $\lim_{k \to \infty} (a_k) = x_0$, the sequence $(f(x_k))$ in $\R^m$ converges to $f(x_0)$. I.e. $\lim_{k \to \infty} f(x_k) = f(\lim_{k \to \infty} a_k)$.

\subsubsection{Rules}
If $f, g$ are continuous then $f + g, f \cdot g, \frac{f}{g}, f \circ g$ are continuous.

\subsubsection{Sandwich Lemma}
If $f, g, h: \R^n \to \R$, where $f(x) < g(x) < h(x) \quad \forall x \in \R^n$ and for some $a \in \R^n, \lim_{y \to a} f(x) = \lim_{x \to a} h(x) = L$ then $\lim_{x \to a} g(x) = L$.

\recipe{Polar Coordinates Trick}
For $f: \R^2 \to \R$ check if continuous at $0$. Let $f(x, y) = f(r \cos \delta, r \sin \delta)$. If $\lim_{(x, y) \to 0} f(x, y) = \lim_{r \to 0}$ is dependant on $\delta \implies$ limes does not exists $\implies$ not continuous at $0$.

\subsection{Partial Derivatives}
For $X \subset \R^n$ open, the function $f: X \to \R^m$ has a partial derivative on $X$ with respect to the $1 \le i \le n$-th variable if $\forall x_0 = (x_{0,1}, \dots x_{0,n}) \in X$ the function $g_i(t):= f(x_{0,1}, \dots x_{0, i - 1}, t, x_{0, i + 1}, \dots x_{0, n})$ on the set $I = \{t \in \R | (x_{0,1}, \dots x_{0, i - 1}, t, x_{0, i + 1}, \dots x_{0, n}) \in X\}$ is differentiable at $t = x_{0, i}$. The derivative $g'(x_{0,i})$ is denoted as $\frac{\partial f}{\partial x_i} (x_0) = \partial{x_i}f(x_0) = \partial_i f(x_0)$.

\begin{compactitem}
\item A partial derivative $\partial_i f$ can be derived again $\partial_{x_j} (\partial_{x_i} f) = \partial_{x_i, x_j} f = \frac{\partial^2 f}{\partial x_i \partial x_j}$. Or for repeated variables $\partial_{x_i} (\partial_{x_i} (f)) = \partial_{x^2_i} f = \frac{\partial^2 f}{\partial x^2_i}$.
\end{compactitem}

\subsubsection{Properties}
For $X \subset \R^n$ open and $f, g: X \to \R^m$ and $1 \le i \le n$:
\begin{compactitem}
    \item If $\partial_i (f)$ and $\partial_i (g)$ exist, then $\partial_i (f + g) = \partial_i (f) + \partial_i (g)$.
    \item If $m = 1$ and if $\partial_i (f)$ and $\partial_i (g)$ exist, then $\partial_i (fg) = \partial_i (f)g + f \partial_i (g)$.
    \item If $m = 1$, if $\partial_i (f)$ and $\partial_i (g)$ exist and if $g(x) \neq 0 \forall x \in X$, then $\partial_i (\frac{f}{g}) = \frac{\partial_i (f)g - f \partial_i (g)}{g^2}$.
\end{compactitem}

\subsubsection{Jacobi Matrix}
For $X \subset \R^n$ open and $f: X \to \R^m$ with partial derivatives on $X$, $f(x) = f_1(x), \dots f_m(x))$, the matrix $J_f(x) = (\partial_j f_i(x))_{\substack{1 \le i \le m\\1 \le j \le n}} =
    \begin{pmatrix}
        \partial_1 f_1 & \partial_2 f_1 & \dots & \partial_n f_1\\
        \partial_1 f_2 & \partial_2 f_2 & \dots & \partial_n f_2\\
        \vdots & \vdots & \ddots & \vdots \\
        \partial_1 f_m & \partial_2 f_m & \dots & \partial_n f_m
    \end{pmatrix}
\quad \forall x \in X$ with $m$ rows and $n$ columns is the Jacobi matrix of $f$ at $x$.

\begin{compactitem}
    \item $\text{det} J_f(x)$ tells when the linear map $f$ is invertible and when not.
\end{compactitem}

\subsubsection{Gradient}
For $X \subset \R^n$ open and $f: X \to \R$ with partial derivatives on $X$, the column vector $
\begin{pmatrix}
    \partial_1 f(x_0)\\
    \vdots\\
    \partial_n f(x_0)
\end{pmatrix}
= (J_f(x_0))^T$ is the gradient of $f$ at $x_0$ denoted as $\bigtriangledown f(x_0)$.

\begin{compactitem}
    \item Slop is maximal in the direction of the gradient.
\end{compactitem}

\subsection{Differentiability}
For $X \subset \R^n$ open, $x_0 \in X$ and $f: X \to \R^m$. $f$ is differentiable at $x_0$ with differential $u: \R^n \to \R^m$ if $\lim_{\substack{x \to x_0\\ x \neq  x_0}} \frac{1}{|x - x_0|}(f(x) - f(x_0) - u(x - x_0)) = \frac{R(x, x_0)}{|x - x_0|} = 0$.
\begin{compactitem}
    \item The linear map $u$ is called \textbf{total differential} of $f$ at $x_0$ and is denoted as $df(x_0)$ or $d_{x_0}f$.
    \item If $f$ is differentiable $\forall x_0 \in X$ then $f$ is differentiable on X.
    \item The map $df(x_0)$ can be represented as a $m \times n$ matrix $Df(x_0)$.
    \item $f(x) = f(x_0) + u(x - x_0) + E(f, x, x_0)$, where $\lim_{\substack{x \to x_0 \\ x \neq x_0}} \frac{E(f, x, x_0)}{|x - x_0|} = 0$
        \begin{compactitem}
            \item $f(x_0) + u(x - x_0)$ is a linear affine function.
        \end{compactitem}
\end{compactitem}

\subsubsection{Properties}
For $X \subset \R^n$ open and $f: X \to \R^m$. $f$ is differentiable at $x_0$:
\begin{compactitem}
    \item $f$ is continuous on $x_0$
        \begin{compactitem}
            \item if $f$ is differentiable on $X$, then $f$ is continuous on $X$
        \end{compactitem}
    \item $f$ has all partial derivatives at $x_0$
    \item $df(x_0)$ is given by $Df(x_0) = J_f(x_0)$
        \begin{compactitem}
            \item if $m = 1$, then $df(x_0) = J_f(x_0) = (\partial_i f(x_0), \dots \partial_n f(x_0))$
        \end{compactitem}
\end{compactitem}

For $X \subset \R^n$ open and $f, g: X \to \R^m$ differentiable on $X$:
\begin{compactitem}
    \item $f + g$ is differentiable and $d(f + g)(x_0) = df(x_0) + dg(x_0)$.
    \item if $m = 1$, then $f \cdot g$ is differentiable
    \item if $m = 1$ and $f(x) \neq 0 \quad \forall x \in X$, then $\frac{f}{g}$ is differentiable
\end{compactitem}

\paragraph{Chain Rule}
For $X \subset \R^n$ open, $Y \subset \R^m$ open and $f: X \to Y$ differentiable on $X$ and $g: Y \to \R^p$ differentiable on $Y$. Then $g \circ f: X \to \R^p$ is differentiable and $d(g \circ f)(x_0) = dg(f(x_0)) \cdot df(x_0)$.

\begin{compactitem}
    \item The Jacobi satisfies $J_{g \circ f}(x_0) = J_g(f(x_0))J_f(x_0)$
\end{compactitem}

\subsubsection{Condition}
For $X \subset \R^n$ open and $f: X \to \R^m$. If $f$ has all partial derivatives and they are continuous on $X$, then $f$ is differentiable in $X$.

\begin{compactitem}
    \item $\substack{\text{partial derivatives exist}\\ + \text{partial derivatives are continuous}} \implies f \text{ is differentiable}$.
    \item $df(x_0) = J_f(x_0)$.
    \item This implies that most functions are differentiable.
\end{compactitem}

\subsubsection{Tangent Space}
For $X \subset \R^n$ open, $f: X \to \R^m$ differentiable on $X$ and $x_0 \in X$, the differential $u = df(x_0)$. The graph of the affine linear approximation $g: \R^n \to \R^m, g(x) = f(x_0) + u(x - x_0)$ is the tangent space of $f$.
\begin{compactitem}
    \item The pts of the tangent space are in $\{(x, y) \in \R^n \times \R^m | y = f(x_0) + u(x - x_0) \}$.
    \item If $m = 1$ then $u(x - x_0) = df(x_0)(x - x_0) = \bigtriangledown f(x_0) \cdot (x - x_0)$.
\end{compactitem}

\recipe{Calculate Tangent Space}
Given function $f: \R^2 \to \R$, calculate the tangent space at pt $(x_0, y_0)$.

\begin{compactenum}
    \item Calculate $\partial_x f$ and $\partial_y f$.
    \item Calculate $f(x_0, y_0)$.
    \item Form $z = f(x_0, y_0) + \partial_x f(x_0, y_0) \cdot (x - x_0) + \partial_y f (x_0, y_0) \cdot (y - y_0)$.
\end{compactenum}

\subsubsection{Directional Derivative}
For $X \subset \R^n$ open, $f: X \to \R^m$, $v \in \R^n$ a non-zero vector and $x_0 \in X$. $f$ has a directional partial derivative $w \in \R^m$ in the direction $v$, if $g(t) = f(x_0 + tv)$, defined on $\{t \in \R | x_0 + tv \in X\}$, holds $g'(0) = w$.

\begin{compactitem}
    \item I.e. $w = \lim_{\substack{t \to 0\\ t \neq 0}} \frac{f(x_0 + tv) - f(x_0)}{t} = \lim_{\substack{t \to 0\\ t \neq 0}} \frac{g(t) - g(0)}{t} = g'(0)$.
    \item Different notations: $w = df(x_0)(v)$
\end{compactitem}

For $X \subset \R^n$ open and $f: X \to \R^m$ differentiable. Then $\forall x \in X$ and $\forall v \in \R^n$ non-zero, $f$ has directional partial derivative at $x_0$ in the direction $v$ and is equal to $df(x_0) (v) = J_f(x_0)(v)$.

\begin{compactitem}
\item If $m = 1$ and $|v| = 1$, then $df(x_0) (v) = J_f(x_0)(v) = \left<\bigtriangledown f(x_0), v\right> = |\bigtriangledown f(x_0)||v|\cos(\theta)$.
    \begin{compactitem}
        \item This is maximal if we maximize $\cos \theta$ which is the case for $\theta = 0$.
    \end{compactitem}
\end{compactitem}

\recipe{Calculate Directional Derivative in Direction $v$}
Given $f: \R^n \to \R$ differentiable, vector $v$ and pt $(x_0, y_0)$. Calculate $df(x_0, y_0)(v)$.

\begin{compactenum}
    \item Calculate all partial derivatives of order $1$.
    \item Form $J_f(x, y)$.
    \item Normalize $\tilde{v} = \frac{v}{|v|}$
    \item Calculate $J_f(x_0, y_0) \cdot \tilde{v}$
\end{compactenum}

\subsubsection{Special Coordinates}
\paragraph{Polar Coordinates}
\begin{inparaitem}
    \item $f: [0, \infty) \times [0, 2\pi) \to \R^2$
    \item $(r, \theta) \mapsto (x, y) = (r \cos \theta, r \sin \theta)$
    \item $J_f(x_0) =
        \begin{pmatrix}
            \frac{\partial f_1}{\partial r} & \frac{\partial f_1}{\partial \theta}\\
            \frac{\partial f_2}{\partial r} & \frac{\partial f_2}{\partial \theta}
        \end{pmatrix} =
        \begin{pmatrix}
            \cos \theta & -r \sin \theta\\
            \sin \theta & r \cos \theta
        \end{pmatrix}$
    \item $\text{det }J_f(r, \theta) = r$
\end{inparaitem}

\paragraph{Cylindrical Coordinates}
\begin{compactitem}
    \item $f: [0, \infty) \times [0, 2 \pi) \times \R \to \R^3$
    \item $(0, \theta, z) \mapsto
        \begin{pmatrix}
            r \cos \theta\\
            r \sin \theta\\
            z
        \end{pmatrix} =
        \begin{pmatrix}
            x\\
            y\\
            z
        \end{pmatrix}$
    \item $J_f(r, \theta) =
        \begin{pmatrix}
            \cos \theta & -r \sin \theta & 0\\
            \sin \theta & r \cos \theta & 0\\
            0 & 0 & 1
        \end{pmatrix}$
    \item $\text{det }J_f = r$
\end{compactitem}

\paragraph{Spherical Coordinates}
\begin{compactitem}
    \item $f: [0, \infty) \times [0, 2 \pi) \times [0, \pi) \to \R^3$
    \item $(r, \theta, \varnothing) \mapsto
        \begin{pmatrix}
            r \cos \theta \sin \varnothing\\
            r \sin \theta \sin \varnothing\\
            r \cos \varnothing
        \end{pmatrix} =
        \begin{pmatrix}
            x\\
            y\\
            z
        \end{pmatrix}$
    \item $J_f(r, \theta, \varnothing) =
        \begin{pmatrix}
            \cos \theta \sin \varnothing & -r \sin \theta \sin \varnothing & r \cos \theta \cos \theta\\
            \sin \theta \sin \varnothing & r \cos \theta \sin \varnothing & r \sin \theta \cos \varnothing\\
            \cos \varnothing & 0 & -r \sin \varnothing
        \end{pmatrix}$
    \item $\text{det } J_f = -r^2 \sin \varnothing$
\end{compactitem}

\subsection{Change of Variable}
For $X \subset \R^n$ open, $f: X \to \R^m$ differentiable and $x_0 \in X$. $f$ is a change of variable around $x_0$ if $\exists$ radius $r > 0$ s.t. the restaction of $f$ to the ball $B = \{x \in \R^n | |x - x_0| < r\}$ of radius $r$ and center $x_0$ has the property that the image $Y = f(B)$ is open in $\R^n$ and $\exists$ differentiable $f: Y \to B$ s.t. $f \circ g = \text{Id}_Y$ and $g \circ f = \text{Id}_B$.

\begin{compactitem}
    \item I.e. $f|_{B_r(x_0)}$ is a bijection to the image with a inverse $f$ is is also differentiable.
\end{compactitem}

\subsubsection{Inverse Function Theorem}
For $X \subset \R^n$ open and $f: X \to \R^m$ differentiable. If $\exists x_0 \in X$ s.t. $J_f(x_0) \neq 0$ the $f$ is a change of variable around $x_0$.

\begin{compactitem}
    \item The Jacobi of $g$ at $x_0$ is $J_g(f(x_0)) = J_f(x_0)^{-1}$.
    \item $\text{det }J_f(x_0) \neq 0 \iff$ Jacobi matrix of $f$ at $x_0$ is invertible.
    \item If $f \in C^k$ then $g \in C^k$.
\end{compactitem}

\subsection{Higher Derivatives}
For $X \subset \R^n$ open and $f: X \to \R^m$
\begin{compactitem}
    \item $f \in C^1$ if $f$ is differentiable on $X$ and all its partial derivatives are continuous.
    \item For $k \ge 2$, $f \in C^k$ is $f$ is differentiable and each $\partial_i f: X \to \R^m \in C^{k-1}$.
    \item If $f \in C^k \quad \forall k \ge 1$ then $f \in C^\infty$ is smooth.
    \item The set of function $g: X \to \R^m \in C^k$ is $C^k(X; R^m)$.
\end{compactitem}

\subsubsection{Notation}
For derivative $\frac{\partial^{k}}{\partial x_1^{m_1} \dots \partial x_n^{m_n}}$ of order $k \le n$ of $f: \R^n \to \R$, we write $\frac{\partial^{|m|}}{\partial x^{m}} f$, with:
\begin{inparaitem}
    \item $m = (m_1, \dots m_n)$ the vector of number of derivatives for each partial derivative.
    \item $|m| := m_1 + \dots + m_n$
    \item $x^n := (x_1^{m_1}, \dots , x_n^{m_n}$
    \item $m! := m_1! \dots m_n!$
\end{inparaitem}

\subsubsection{Mixed Derivates Commute}
For $X \subset \R^n$ open, $f: X \to \R^m \in C^k$ and $k \ge 2$. The derivatives of order $k$ are independent of the order in which the partial derivatives are taken.

\begin{compactitem}
    \item I.e. $\partial_{x, y} f = \partial_{y, x} f$
\end{compactitem}

\subsubsection{Hessian}
For $X \subset \R^n$ open and $f: X \to \R^m \in C^2$. The Hessian matrix of $x$ is $\text{Hess}_f(x) = H_f(x) = (\partial_{x_i, x_j}f)_{1 \le i, j \le n} =
\begin{pmatrix}
    \partial_{x_1^2} f & \partial_{x_1, x_2} f & \dots & \partial_{x_1, x_n} f\\
    \partial_{x_2, x_1} f & \partial_{x_2^2} f & \dots & \partial_{x_2, x_n} f\\
    \vdots & & \ddots & \vdots\\
    \partial_{x_n, x_1} f & \partial{x_n, x_2} f & \dots & \partial_{x_n^2} f
\end{pmatrix}$.

\begin{compactitem}
    \item If partial derivatives commute, then $H_f(x)$ is symmetric, square, $n \times n$.
\end{compactitem}

\subsection{Taylor Approximation}
\subsubsection{Order 1}
For $f: \R^n \to \R$, the Taylor Polynom of $f$ at $x_0$ of degree $1$ is $T_1f(y; x_0) := f(x_0) + \bigtriangledown f(x_0) \cdot y$.

For a point $x_0$ close to $x$ the approximation of $x$ by a affine linear polynomial is $T_1(x - x_0; x_0)$.

\begin{compactitem}
    \item $T_1$ is equivalent to the affine linear approximation of $f$ we have seen before.
\end{compactitem}

\subsubsection{Order k}
For $X \subset \R^n, x_0 \in X, f: X \to \R, f \in C^k$, the $k$-th Taylor polynomial of $f$ at $x_0$ is $T_k(y; x_0) = f(x_0) + \sum_{i=1}^{n} \frac{\partial f}{\partial x_i} (x_0) y_i + \dots + \sum_{m_1 + \dots + m_n = k} \frac{1}{m_1! \dots m_n!}\frac{\partial^k f}{\partial x_1^{m_1} \dots \partial x_n^{m_n}}(x_0)y_1^{m_1} \dots y_n^{m_n} = \sum_{|m| \le k} \frac{1}{m!} \partial_x^m f(x_0) y^m$. Where $y^m := (y_1^{m_1}, \dots , y_n^{m_n})$.

\subsubsection{Approximation}
For $f \in C^k(X; \R), x_0 \in X$. $T_k$ is approximation of $f$ at $x$ with $x_0$: $f(x) = T_kf(x - x_0; x_0) + E_k(f, x, x_0)$. For the error $E_k$ it holds $\lim_{x \to x_0} \frac{E_k(f, x, x_0)}{|x - x_0|^k} = 0$.

\recipe{Taylor Polynom for $f: \R^2 \to \R$}
Give Taylor of order $k \in \{1, 2, 3\}$ of $f(x, y): \R^2 \to \R$ for a pt $(x_0, y_0)$.

\begin{compactitem}
    \item Computer all parietal derivatives of order up to $k$.
    \item Form Taylor according to:
\end{compactitem}

\begin{compactitem}
    \item $T_1 f((\alpha, \beta); (x_0, y_0)) = f(x_0, y_0) + \frac{\partial f(x_0, y_0)}{\partial x} \cdot \alpha + \frac{\partial f(x_0, y_0)}{\partial y} \cdot \beta$
    \item $T_2 f((\alpha, \beta); (x_0, y_0)) = T_1 f((\alpha, \beta); (x_0, y_0)) + \frac{1}{2} \frac{\partial^2 f(x_0, y_0)}{\partial x \partial x} \cdot \alpha^2 + \frac{1}{2} \frac{\partial^2 f(x_0, y_0)}{\partial y \partial y} \cdot \beta^2 + \frac{\partial^2 f(x_0, y_0)}{\partial x \partial y} \cdot \alpha\beta$
    \item $T_3 f((\alpha, \beta); (x_0, y_0)) = T_2 f((\alpha, \beta); (x_0, y_0)) + \frac{1}{6} \frac{\partial^3 f(x_0, y_0)}{\partial^3 x} \alpha^3 + \frac{1}{2} \frac{\partial^3 f(x_0, y_0)}{\partial^2 x \partial y} \alpha^2 \beta + \frac{1}{2} \frac{\partial^3 f(x_0, y_0)}{\partial x \partial^2 y} \alpha \beta^2 + \frac{1}{6} \frac{\partial^3 f(x_0, y_0)}{\partial^3 y} \beta^3$
\end{compactitem}

\recipe{Approximate $f: \R^2 \to \R$}
Given $f: \R^2 \to \R$ and pt $(x, y)$. Approximate $f(x, y)$ using Taylor.

\begin{compactitem}
    \item Find pt $(x_0, y_0) \approx (x, y)$ which approximates $(x, y)$ and is easy to evaluate.
    \item Form Taylor $T_k((x - x_0, y - y_0); (x_0, y_0))$ of desired order $k$.
\end{compactitem}

\subsection{Critical Points}
\subsubsection{Extrema}
For $X \subset \R^n$ open and $f: X \to \R$ differentiable. We define a neighbourhood of $x_0$ as $B_r(x_0) = \{x \in \R^n | |x - x_0| < r\} \subset X$. If $x_0 \in X$:

\begin{compactdesc}
    \item[Local Maximum:] at $x_0$ is $f(x) \le f(x_0) \quad \forall x \in B_r(x_0)$.
    \item[Local Minimum:] at $x_0$ is $f(x) \ge f(x_0) \quad \forall x \in R_r(x_0)$.
\end{compactdesc}

In that case:
\begin{inparaitem}
    \item  $df(x_0) = 0$
    \item $\bigtriangledown f(x_0) = 0$
    \item $\frac{\partial f}{\partial x_i}(x_0) = 0, 1 \le i \le n$
\end{inparaitem}

\subsubsection{Critical Point}
For $X \subset \R^n$ open and $f: X \to \R$ differentiable. $x_0 \in X$, $\bigtriangledown f(x_0) = 0$ is a critical point.

\begin{compactitem}
    \item Critical points are candidates for local extrema.
    \item Critical points which are not a local extrema are saddle points.
\end{compactitem}

\subsubsection{Global Extrema}
For $\bar{X} = \text{innerPart}(X) \cup \text{boundary}(X) \subset \R^n$ compact (closed and bounded) and $f: \bar{X} \to \R$ differentiable. A global extrema exists and it is either at a critical point ($\in \text{innterPart}(X)$) or on the boundary ($\in \text{boundary}(X)$) of $\bar{X}$.

\subsubsection{Non-Degenerate Critical Point}
For $X \subset \R^n$ open and $f: X \to \R \in C^2$. A critical point $x_0 \in X$ is called non-degenerate if $\text{det }\text{Hess}_f(x) \neq 0$.

\subsubsection{Definite/Indefinite}
A symmetric matrix $A = (a_{i,j}) \in \R^{n \times n}$, with $\text{det } A \neq 0$ is:

\begin{compactdesc}
    \item[Positive Definite ($\textbf{A > 0}$):] iff $xAx^t > 0 \forall x \in \R^n \setminus \{0\}$
        \begin{compactitem}
            \item All eigenvalues are positive.
        \end{compactitem}
    \item[Neg. Definite ($\textbf{A < 0}$):] iff $xAx^t < 0 \forall x \in \R^n \setminus \{0\}$
        \begin{compactitem}
            \item All eigenvalues are negative.
        \end{compactitem}
    \item[Indefinite:] otherwise
        \begin{compactitem}
            \item Mixed eigenvalues.
        \end{compactitem}
\end{compactdesc}

\paragraph{Criteria}
$A$ is positive definite $\iff \text{det }A_j > 0 \quad \forall 1 \le j \le n$ and $A_j = (a_{k,l})_{\substack{1 \le k \le j\\ 1 \le l \le j}}$

\subsubsection{Local Min/Max}
For $X \subset \R^n$ open, $f: X \to \R \in C^2$ and $x_0 \in X$ a non-degenerate critical point of $f$ ($\bigtriangledown f(x_0) = 0, \text{det Hess}_f (x_0) \neq 0$). Then:
\begin{compactitem}
    \item If $\text{Hess}_f (x_0) > 0 \implies x_0$ is a local minimum.
    \item If $\text{Hess}_f (x_0) < 0 \implies x_0$ is a local maximum.
    \item If $\text{Hess}_f (x_0)$ is indefinite $\implies x_0$ is saddle point.
\end{compactitem}

In case that $\text{Hess}_f (x_0) = 0$ (i.e. $x_0$ is degenerate) we cannot use this criteria.

\recipe{Extreme Pts using $2$nd Derivative Method (open)}
Given $f: \R^n \to \R$ find extreme points and determine if they are max/min/saddle.
\begin{compactenum}
    \item Calculate all partial derivatives up to order two.
    \item Form $df(x) = (\partial_{x_1}f, \dots \partial_{x_n}f)$
    \item Find all zeros $x_0$ of $df(x_0)$. They are the critical pts.
    \item If $X \subset \R^n$, check boundary for max/min pts.
    \item Form $\text{Hess}_f (x)$
    \item Calculate $\text{det Hess}_f (x)$
    \item Insert all $x_0$ into $\text{det Hess}_f (x_0)$ and evaluate.
    \item If $\text{det Hess}_f (x_0) = 0 \implies$ this method does not work.
    \item Use previous theorem to determine if $x_0$ is min/max/saddle.
\end{compactenum}

\recipe{Extreme Pts using $2$nd Derivative Method (closed)}
As before, but $f: X \to \R$.

\begin{compactenum}
    \item Follow previous recipe
    \item Evaluate all corners and record their value.
    \item For site $i$:
        \begin{compactenum}
            \item Find parametrisation $\gamma_i(t)$.
            \item Calculate $\frac{d \gamma_i}{d t}$.
            \item Find $t_0$ for which $\frac{d \gamma_i}{d t} = 0$.
            \item Calculate $f(\gamma_i(t_0))$ and record their values.
        \end{compactenum}
    \item Compare all evaluated points and determine the min/max/saddle pts.
\end{compactenum}

\recipe{$\text{det } A$ with Sarrus}
Given matrix $A: 3 \times 3$ calculate $\text{det }A$.

$\text{det } A = A_{1, 1} A_{2, 2} A_{3, 3} + A_{2, 1} A_{3, 2} A_{1, 3} + A_{3, 1} A_{1, 2} A_{2, 3} - A_{1, 3} A_{2, 2} A_{3, 1} - A_{1, 2} A_{2, 1} A_{3, 3} - A_{1, 1} A_{2, 3} A_{3, 2}$

\recipe{Eigenwerte}
Given square matrix $A$. Find the eigenvalues of $A$.

\begin{compactenum}
    \item Form characteristic polynomial $\text{det } (A - \lambda I)$.
    \item Find zeros of the characteristic polynomial. They are the eigenvalues.
\end{compactenum}

